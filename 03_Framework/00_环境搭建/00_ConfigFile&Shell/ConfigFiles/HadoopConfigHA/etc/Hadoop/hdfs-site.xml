<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!--	nameNode数据存储路径-->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://${hadoop.data.dir}/name</value>
    </property>
    <!--	dataNode数据存储路径-->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://${hadoop.data.dir}/data</value>
    </property>

    <!--	HDFS HA集群配置  start  -->

    <!--	hadoop nameService-->
    <property>
        <name>dfs.nameservices</name>
        <value>mycluster</value>
    </property>

    <!--	hadoop nameNode集群列表-->
    <property>
        <name>dfs.ha.namenodes.mycluster</name>
        <value>nn1,nn2, nn3</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn1</name>
        <value>hadoop102:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn2</name>
        <value>hadoop103:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn3</name>
        <value>hadoop104:8020</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster.nn1</name>
        <value>hadoop102:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster.nn2</name>
        <value>hadoop103:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster.nn3</name>
        <value>hadoop104:9870</value>
    </property>

    <!--存储Edits的qj集群配置-->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster</value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>${hadoop.data.dir}/jn</value>
    </property>

    <property>
        <name>dfs.client.failover.proxy.provider.mycluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

    <!--	在故障转移时，关闭Active状态NameNode的方法，此处配置的是SSH登录-->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <!--	ssh登录需要的密钥-->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/atguigu/.ssh/id_rsa</value>
    </property>

    <!--    HDFS HA 自动故障转移-->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>


    <!-- 2NN配置(Hadoop HA集群，配置了三个NN，2NN不需要了) -->
    <!--
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop104:9868</value>
    </property>
    -->

    <!--	HDFS HA集群配置  end  -->

    <!-- HDFS文件块的大小，hadoop2.x默认是128M -->
    <property>
        <name>dfs.blockSize</name>
        <value>128m</value>
    </property>

    <!-- DataNode超时时间配置，单位：毫秒-->
    <property>
        <name>dfs.namenode.heartbeat.recheck-interval</name>
        <value>300000</value>
    </property>

    <!-- DataNode心跳检查时间，单位：秒-->
    <property>
        <name>dfs.heartbeat.interval</name>
        <value>3</value>
    </property>

    <!-- 白名单文件设置 -->
    <property>
        <name>dfs.hosts</name>
        <value>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts</value>
    </property>

    <!-- 黑名单文件设置 -->
    <property>
        <name>dfs.hosts.exclude</name>
        <value>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude</value>
    </property>

    <!-- DataNode多目录配置 -->
    <!--
    <property>
            <name>dfs.datanode.data.dir</name>
    <value>file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2</value>
    </property>
    -->
</configuration>
